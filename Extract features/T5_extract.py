import re
import torch
import numpy as np
from transformers import T5Tokenizer, T5EncoderModel
from Bio import SeqIO

# é…ç½®å‚æ•°
MAX_SEQ_LENGTH = 300  # æœ€å¤§å¤„ç†åºåˆ—é•¿åº¦
MODEL_NAME = "Rostlab/prot_t5_xl_uniref50"
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def initialize_model():
    """åˆå§‹åŒ–æ¨¡å‹ï¼ˆä¿æŒå…¨ç²¾åº¦ï¼‰"""
    print("ğŸ”„ åˆå§‹åŒ–æ¨¡å‹ä¸­...")
    tokenizer = T5Tokenizer.from_pretrained(MODEL_NAME, legacy=False)
    model = T5EncoderModel.from_pretrained(MODEL_NAME)
    model = model.to(DEVICE).eval()  # ä¿æŒFP32ç²¾åº¦
    print(f"âœ… æ¨¡å‹å·²åŠ è½½åˆ° {DEVICE}ï¼ˆFP32æ¨¡å¼ï¼‰")
    return tokenizer, model


def process_sequence(tokenizer, model, raw_seq):
    """å…¨ç²¾åº¦å¤„ç†å•ä¸ªåºåˆ—"""
    try:
        # é¢„å¤„ç†é˜¶æ®µ
        clean_seq = raw_seq[:MAX_SEQ_LENGTH].upper()  # å¼ºåˆ¶æˆªæ–­
        clean_seq = re.sub(r"[UZOB]", "X", clean_seq)

        # åŠ¨æ€é•¿åº¦è®¡ç®—
        seq_length = min(len(clean_seq), MAX_SEQ_LENGTH)
        effective_max_length = seq_length + 2  # åŒ…å«ç‰¹æ®Štoken

        # ç”Ÿæˆè¾“å…¥ï¼ˆä¿æŒæ•°æ®ç±»å‹æ­£ç¡®ï¼‰
        inputs = tokenizer(
            " ".join(list(clean_seq)),
            add_special_tokens=True,
            padding="max_length",
            max_length=MAX_SEQ_LENGTH + 2,
            truncation=True,
            return_tensors="pt"
        )

        # ç¡®ä¿æ•°æ®ç±»å‹æ­£ç¡®
        input_ids = inputs["input_ids"].long().to(DEVICE)  # å¿…é¡»ä¿æŒLongç±»å‹
        attention_mask = inputs["attention_mask"].float().to(DEVICE)  # ä¿æŒFP32

        # æ˜¾å­˜ä¼˜åŒ–å‰æ£€æŸ¥
        if DEVICE.type == "cuda":
            torch.cuda.empty_cache()
            print(f"â³ å½“å‰æ˜¾å­˜å ç”¨: {torch.cuda.memory_allocated() / 1024 ** 3:.2f} GB")

        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = model(
                input_ids=input_ids,
                attention_mask=attention_mask
            )

        # ç‰¹å¾åå¤„ç†
        embeddings = outputs.last_hidden_state[0][1:-1].cpu().numpy()

        # åŠ¨æ€å¡«å……
        if embeddings.shape[0] < MAX_SEQ_LENGTH:
            padding = np.zeros((MAX_SEQ_LENGTH - embeddings.shape[0], 1024))
            embeddings = np.vstack([embeddings, padding])

        return embeddings

    except Exception as e:
        print(f"âŒ å¤„ç†é”™è¯¯: {str(e)}")
        return None


def main():
    """ä¸»å¤„ç†æµç¨‹"""
    tokenizer, model = initialize_model()

    # è¯»å–æ•°æ®
    input_path = "/home/ys/sunhuaiyang/predict/case_study/10.fasta"
    output_path = "/home/ys/sunhuaiyang/predict/case_study/feature/10T5_features.npy"

    print("\nğŸ“– è¯»å–FASTAæ–‡ä»¶ä¸­...")
    sequences = [str(rec.seq) for rec in SeqIO.parse(input_path, "fasta")]
    print(f"âœ… æˆåŠŸåŠ è½½ {len(sequences)} æ¡åºåˆ—")

    # å¤„ç†æ§åˆ¶
    features = []
    for idx, seq in enumerate(sequences):
        seq_len = len(seq)
        print(f"\nğŸ” å¤„ç†è¿›åº¦ {idx + 1}/{len(sequences)} | å½“å‰åºåˆ—é•¿åº¦: {seq_len}")

        # æ˜¾å­˜ä¿æŠ¤æœºåˆ¶
        if seq_len > 300 and DEVICE.type == "cuda":
            print("âš ï¸ æ£€æµ‹åˆ°é•¿åºåˆ—ï¼Œå¯ç”¨æ˜¾å­˜ä¿æŠ¤æ¨¡å¼")
            torch.cuda.empty_cache()

        # å¤„ç†åºåˆ—
        emb = process_sequence(tokenizer, model, seq)

        if emb is not None:
            # ç»´åº¦éªŒè¯
            assert emb.shape == (MAX_SEQ_LENGTH, 1024), f"ç»´åº¦å¼‚å¸¸: {emb.shape}"
            features.append(emb)
            print(f"ğŸŸ¢ ç‰¹å¾ç”ŸæˆæˆåŠŸ | ç»´åº¦: {emb.shape}")

    # ä¿å­˜ç»“æœ
    np.save(output_path, np.array(features))
    print(f"\nğŸ’¾ ç‰¹å¾å·²ä¿å­˜è‡³: {output_path}")
    print(f"æœ€ç»ˆç»´åº¦: {np.array(features).shape}")

    exit()


if __name__ == "__main__":
    main()